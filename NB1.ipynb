{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9m6GJzKCn0/t7zMAsSz2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nihir2904/Advanced_NLP/blob/main/NB1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer Basics\n",
        "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your corpus (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called tokens and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells.\n",
        "\n",
        "## Generating the vocabulary\n",
        "In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the fit_on_texts() method and you can get the result by looking at the word_index property. More frequent words have a lower index."
      ],
      "metadata": {
        "id": "Czpr-csb5Hvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3urSfoAc4zch",
        "outputId": "7e8a7ab8-f91e-47db-cc90-f6650ce3c385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Define input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat!'\n",
        "    ]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100) # the num_words parameter is just used to represent the limit of words in our dictionary, gives most common 100 words\n",
        "\n",
        "# Generate indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The num_words parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. The important thing to note is it does not affect how the word_index dictionary is generated. You can try passing 1 instead of 100 as shown on the next cell and you will arrive at the same word_index.\n"
      ],
      "metadata": {
        "id": "FJdOWn7P5nRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDTJULSr5XJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text to Sequences\n",
        "In the previous lab, you saw how to generate a word_index dictionary to generate tokens for each word in your corpus. You can then use the result to convert each of the input sentences into a sequence of tokens. That is done using the texts_to_sequences() method as shown below."
      ],
      "metadata": {
        "id": "CiXS3rC_dVmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your input texts\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "\n",
        "# Tokenize the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate list of token sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM2t_0_-dt0Q",
        "outputId": "cda0eabd-fe53-4b67-9299-2d2c29a0d850"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Padding\n",
        "you will usually need to pad the sequences into a uniform length because that is what your model expects. You can use the pad_sequences for that. By default, it will pad according to the length of the longest sequence. You can override this with the maxlen argument to define a specific length."
      ],
      "metadata": {
        "id": "-N45wnKyeBqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences to a uniform length\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW--IRIVeGG8",
        "outputId": "e4b08fcd-a5f3-4dbf-a198-34112b862d82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded Sequences:\n",
            "[[ 0  5  3  2  4]\n",
            " [ 0  5  3  2  7]\n",
            " [ 0  6  3  2  4]\n",
            " [ 9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Out-of-vocabulary tokens\n",
        "This will be used when you have input words that are not found in the word_index dictionary. For example, you may decide to collect more text after your initial training and decide to not re-generate the word_index. You will see this in action in the cell below. The token 1 is inserted for words that are not found in the dictionary."
      ],
      "metadata": {
        "id": "ZcIfLrh4eOs7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZLN5ahUeKJs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-vTOiZkejkt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}